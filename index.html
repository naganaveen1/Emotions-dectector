<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fast Emotion Detector</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            overflow: hidden;
            background-color: #0f172a; /* Dark slate */
        }
        #video-container {
            position: relative;
            width: 100%;
            max-width: 640px; /* Slightly smaller for faster processing */
            aspect-ratio: 16 / 9;
            margin: auto;
            background-color: #000;
            border-radius: 0.75rem;
        }
        #video, #overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border-radius: 0.75rem;
        }
        #video {
            object-fit: cover;
            transform: scaleX(-1); /* Mirror effect */
        }
        #overlay {
             transform: scaleX(-1); /* Mirror canvas to match video */
        }
        .emotion-emoji {
            font-size: 3rem;
            line-height: 1;
        }
        /* Loading Spinner */
        .loader {
            border: 5px solid #384252; /* Lighter slate */
            border-top: 5px solid #3498db; /* Blue */
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto 10px auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .hidden { display: none; }
    </style>
</head>
<body class="bg-slate-900 text-slate-100 flex flex-col items-center justify-center min-h-screen p-4 selection:bg-sky-500 selection:text-white">

    <div class="w-full max-w-xl bg-slate-800 bg-opacity-60 backdrop-blur-lg shadow-2xl rounded-xl p-6 md:p-8 space-y-5 text-center">
        <h1 class="text-3xl font-bold text-sky-400">Emotion Detector AI</h1>
        <p class="text-slate-400">Look into the camera and see the magic!</p>

        <div id="video-container" class="shadow-lg rounded-xl overflow-hidden">
            <video id="video" autoplay muted playsinline></video>
            <canvas id="overlay"></canvas>
        </div>

        <div id="status-container" class="py-4 space-y-3">
            <div id="loading-container" class="text-sky-300">
                <div class="loader"></div>
                <p id="loading-text">Initializing...</p>
            </div>
            <div id="emotion-display" class="hidden">
                <p class="text-xl font-semibold text-slate-300">Detected Emotion:</p>
                <p id="emotion-text" class="text-4xl font-bold text-white mt-1">...</p>
                <p id="emotion-emoji" class="emotion-emoji mt-2">ðŸ¤”</p>
            </div>
            <div id="error-message" class="text-red-400 font-semibold hidden"></div>
        </div>

        <p class="text-xs text-slate-500 pt-3">
            Powered by <a href="https://github.com/vladmandic/face-api" target="_blank" rel="noopener noreferrer" class="hover:text-sky-400 underline">face-api.js</a>. Privacy: All processing is local.
        </p>
    </div>

    <script>
        const video = document.getElementById('video');
        const overlayCanvas = document.getElementById('overlay');
        const loadingContainer = document.getElementById('loading-container');
        const loadingText = document.getElementById('loading-text');
        const emotionDisplay = document.getElementById('emotion-display');
        const emotionText = document.getElementById('emotion-text');
        const emotionEmoji = document.getElementById('emotion-emoji');
        const errorMessage = document.getElementById('error-message');
        const videoContainer = document.getElementById('video-container');

        // Corrected MODEL_URL for @vladmandic/face-api
        const MODEL_URL = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model';

        const emotionMap = {
            neutral: { text: 'Neutral', emoji: 'ðŸ˜' },
            happy: { text: 'Happy', emoji: 'ðŸ˜„' },
            sad: { text: 'Sad', emoji: 'ðŸ˜¢' },
            angry: { text: 'Angry', emoji: 'ðŸ˜ ' },
            fearful: { text: 'Fearful', emoji: 'ðŸ˜¨' },
            disgusted: { text: 'Disgusted', emoji: 'ðŸ¤¢' },
            surprised: { text: 'Surprised', emoji: 'ðŸ˜²' }
        };

        function updateLoadingText(text) {
            console.log("Status:", text);
            loadingText.textContent = text;
        }

        function showError(message) {
            console.error("Error:", message);
            loadingContainer.classList.add('hidden');
            emotionDisplay.classList.add('hidden');
            errorMessage.textContent = message;
            errorMessage.classList.remove('hidden');
        }

        async function loadModels() {
            updateLoadingText("Checking AI library...");
            if (typeof faceapi === 'undefined') {
                showError("Core AI library failed to load. Please check your internet connection or ad-blockers and refresh.");
                return false;
            }
            try {
                updateLoadingText("Loading AI Models (Tiny)...");
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
                    faceapi.nets.faceLandmark68TinyNet.loadFromUri(MODEL_URL), // Uses TinyNet for landmarks
                    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
                ]);
                updateLoadingText("Models Loaded!");
                return true;
            } catch (error) {
                showError(`Failed to load AI models. Please refresh. (${error.message}, URL: ${MODEL_URL})`);
                return false;
            }
        }

        async function startVideo() {
            updateLoadingText("Requesting Camera Access...");
            try {
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    showError("Camera API not supported. Please use a modern browser.");
                    return false;
                }
                const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
                video.srcObject = stream;
                updateLoadingText("Starting Camera Feed...");
                return new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        console.log("Camera feed started.");
                        resolve(true);
                    };
                    video.onerror = () => {
                         showError("Error with video feed.");
                         resolve(false);
                    }
                });
            } catch (err) {
                if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
                    showError("Camera access denied. Please allow camera access and refresh.");
                } else if (err.name === "NotFoundError" || err.name === "DevicesNotFoundError") {
                    showError("No camera found. Please connect a camera.");
                } else {
                    showError(`Could not access camera. (${err.name})`);
                }
                return false;
            }
        }

        function updateEmotionDisplay(detections) {
            if (detections && detections.length > 0) {
                const expressions = detections[0].expressions;
                let dominantEmotion = 'neutral';
                let maxScore = 0;
                for (const [emotion, score] of Object.entries(expressions)) {
                    if (score > maxScore) {
                        maxScore = score;
                        dominantEmotion = emotion;
                    }
                }
                const emotionData = emotionMap[dominantEmotion] || emotionMap.neutral;
                emotionText.textContent = emotionData.text;
                emotionEmoji.textContent = emotionData.emoji;
            } else {
                emotionText.textContent = "Searching...";
                emotionEmoji.textContent = "ðŸ¤”";
            }
        }

        function startDetection() {
            const displaySize = { width: videoContainer.clientWidth, height: videoContainer.clientHeight };
            faceapi.matchDimensions(overlayCanvas, displaySize);
            const context = overlayCanvas.getContext('2d', { willReadFrequently: true }); 

            // Using TinyFaceDetectorOptions with a smaller input size for potentially faster processing
            const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.5 });

            const detect = async () => {
                if (video.paused || video.ended || !video.srcObject || video.readyState < 3) { // Added more checks
                    requestAnimationFrame(detect); // Keep trying if video not fully ready
                    return;
                }

                const detections = await faceapi.detectAllFaces(video, options)
                    .withFaceLandmarks(true) // Use true for the tiny model
                    .withFaceExpressions();

                const resizedDetections = faceapi.resizeResults(detections, displaySize);

                context.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
                // Optional: Draw detections - good for debugging, but can impact performance
                // faceapi.draw.drawDetections(overlayCanvas, resizedDetections);
                // faceapi.draw.drawFaceLandmarks(overlayCanvas, resizedDetections);
                // faceapi.draw.drawFaceExpressions(overlayCanvas, resizedDetections, 0.05);

                updateEmotionDisplay(resizedDetections);
                requestAnimationFrame(detect);
            };

            detect(); // Start the loop
        }

        async function initializeApp() {
            const modelsOk = await loadModels();
            if (!modelsOk) return;

            const videoOk = await startVideo();
            if (!videoOk) return;

            video.addEventListener('play', () => {
                console.log("Video play event triggered.");
                loadingContainer.classList.add('hidden');
                errorMessage.classList.add('hidden');
                emotionDisplay.classList.remove('hidden');
                startDetection();
            });
            
            video.addEventListener('loadeddata', () => { // Added listener for loadeddata
                console.log("Video data loaded.");
                 // Attempt to play if not already playing (addresses some autoplay policies)
                if (video.paused) {
                    video.play().catch(e => {
                        console.warn("Autoplay blocked on loadeddata:", e);
                        showError("Click the video or allow autoplay to start.");
                    });
                }
            });


            // Ensure video tries to play if not already handled by autoplay
            try {
                 if(video.paused) { // Only try to play if it's paused
                    await video.play();
                 }
            } catch (playError) {
                 console.warn("Initial autoplay attempt blocked:", playError);
                 // The 'play' event listener or a user interaction will be needed.
                 // No error shown here as 'play' listener will handle it or user needs to interact.
            }
        }

        // Adjust canvas on resize
        window.addEventListener('resize', () => {
            if (video.srcObject && video.readyState > 0) { // Check if video is active
                const displaySize = { width: videoContainer.clientWidth, height: videoContainer.clientHeight };
                faceapi.matchDimensions(overlayCanvas, displaySize);
            }
        });

        // Start everything when the DOM is ready
        document.addEventListener('DOMContentLoaded', initializeApp);
    </script>
</body>
</html>
