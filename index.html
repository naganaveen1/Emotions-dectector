<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Emotion Detection</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/react/18.2.0/umd/react.production.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/react-dom/18.2.0/umd/react-dom.production.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/babel-standalone/7.18.9/babel.min.js"></script>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      background: linear-gradient(135deg, #1a202c, #2d3748);
      min-height: 100vh;
      display: flex;
      justify-content: center;
      align-items: center;
      color: white;
    }
    canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
  </style>
</head>
<body>
  <div id="root"></div>
  <script type="text/babel">
    const { useState, useEffect, useRef } = React;

    const App = () => {
      const videoRef = useRef(null);
      const canvasRef = useRef(null);
      const [emotion, setEmotion] = useState(null);
      const [confidence, setConfidence] = useState(null);
      const [isLoading, setIsLoading] = useState(true);
      const [error, setError] = useState(null);

      useEffect(() => {
        const loadModelsAndStartVideo = async () => {
          try {
            // Load face-api.js models
            await Promise.all([
              faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/models'),
              faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/models'),
              faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/models'),
            ]);

            // Request camera access
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            videoRef.current.srcObject = stream;
            setIsLoading(false);

            // Start emotion detection
            videoRef.current.addEventListener('play', () => {
              const canvas = canvasRef.current;
              const displaySize = { width: videoRef.current.width, height: videoRef.current.height };
              faceapi.matchDimensions(canvas, displaySize);

              setInterval(async () => {
                const detections = await faceapi
                  .detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions())
                  .withFaceLandmarks()
                  .withFaceExpressions();

                if (detections.length > 0) {
                  const expressions = detections[0].expressions;
                  const maxExpression = Object.keys(expressions).reduce((a, b) =>
                    expressions[a] > expressions[b] ? a : b
                  );
                  setEmotion(maxExpression.charAt(0).toUpperCase() + maxExpression.slice(1));
                  setConfidence((expressions[maxExpression] * 100).toFixed(2));
                } else {
                  setEmotion(null);
                  setConfidence(null);
                }

                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                faceapi.draw.drawDetections(canvas, resizedDetections);
                faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
              }, 100);
            });
          } catch (err) {
            setError('Failed to access camera or load models. Please ensure camera access is granted.');
            setIsLoading(false);
          }
        };

        loadModelsAndStartVideo();
      }, []);

      return (
        <div className="flex flex-col items-center justify-center min-h-screen p-4">
          <h1 className="text-4xl font-bold mb-6 text-white drop-shadow-lg">
            Emotion Detection
          </h1>
          {isLoading && (
            <div className="text-xl text-gray-300 animate-pulse">Loading models...</div>
          )}
          {error && (
            <div className="text-red-500 text-lg mb-4">{error}</div>
          )}
          <div className="relative bg-gray-800 rounded-lg shadow-lg overflow-hidden">
            <video
              ref={videoRef}
              autoPlay
              muted
              width="640"
              height="480"
              className="rounded-lg"
            ></video>
            <canvas ref={canvasRef} className="absolute top-0 left-0" />
          </div>
          {emotion && confidence && (
            <div className="mt-6 p-4 bg-gray-900 rounded-lg shadow-lg text-center">
              <p className="text-2xl text-white">
                Detected Emotion: <span className="font-bold text-blue-400">{emotion}</span>
              </p>
              <p className="text-lg text-gray-300">
                Confidence: <span className="font-bold">{confidence}%</span>
              </p>
            </div>
          )}
          {!emotion && !isLoading && !error && (
            <div className="mt-6 text-lg text-gray-300">
              No face detected. Please ensure your face is visible.
            </div>
          )}
        </div>
      );
    };

    ReactDOM.render(<App />, document.getElementById('root'));
  </script>
</body>
</html>